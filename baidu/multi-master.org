#+TITLE: Master多机化的开发

* 前言
Master多机化预计于今天（<2014-03-10 一>）开始正式进入开发阶段，比预期推迟了两周。
我计划在两周后完成开发并上线单台机器。

本篇日志按照开发进展来组织，记录下开发过程中的思考和开发进度，留作以后用（我不再
使用Evernote来记录这种 *开发进展类的笔记* 了，而是和其他笔记一样和使用Emacs
org-mode）。

* Master多机化的升级路线
Master多机化的新增特性非常多，如支持扩容缩容，分配传输任务，主从切换，负载均衡，
故障迁移，多Master下日志元信息的管理以及监控信息的汇总和展示等。所以， *必须要设
计好一个合理稳妥的升级路线* ，否则项目风险极大。制定升级路线以及做升级开发时应遵
循三个原则：
1. 稳步迭代
2. 持续发布和部署
3. 这是个 *重构* 原有代码和原有单测的好机会

** 第一步：抽取ConfigManager和FlowManager
从当前Master中抽取出ConfigManager和FlowManager来。让ConfigManager接管所有日志的新
增/修改/删除，以及为FlowManager分配日志传输任务的功能。让FlowManager只管理被
ConfigManager分配的日志模块。有几点要注意：
1. ConfigManager通过写FlowManager所在机器再ZK上的节点（以ip:port命名）来告知其管
   理的日志模块id列表
1. FlowManager任然可以获取日志配置，但是它只能获取它所管理的日志的配置
2. MinosMeta要提供一个新接口，能获取ConfigManager分配给某个FlowManager的日志模块
   的id列表
3. FlowManager的各种管理操作：包括Update（传输流的新建/修改/销毁），Save（更新
   Checkpoint到HDFS），Fallback（处理故障节点），Notify（通知下游数据系统），报警
   （传输延时的报警）都只调用FlowManager暴露出来的获取日志模块id的列表

这样改造后，能将Master的大部分针对传输流的管理操作做到与Master多机化无关，从而降
低风险。改造后，Master的架构大致如下：

##+begin_src ditaa :file multi-master-1.png
	     +---------------+	             +--------------+
       	     | ConfigManager |  read/write   |   ZooKeeper  |
             | cBLU	     +<------------->+	 c707       |
             |               |               |              |
             +---------------+ 	             +------+-------+
	    			       	       	    ^
	    			   	       	    | read/write
	      			   		    v
       	     +--------------------------------------+-------------------------+
  	     | 		       	        FlowManager    	       	       	      |
  	     | 				cGRE   	       	       	       	      |
       	     | +-------+ +-----+ +----------+ +--------+ +-------+ +--------+ |
  	     | |Updater| |Saver| |Fallbacker| |Notifier| |Alarmer| |....    | |
             | |cGRE   | |cGRE | |cGRE      | |cGRE    | |cGRE   | |cGRE    | |
       	     | +-------+ +-----+ +----------+ +--------+ +-------+ +--------+ |
       	     +----------------------------------------------------------------+
##+end_src

** 第二步：CentralMaster初现，支持静态分配日志传输任务，初步实现Master多机化
完成将ConfigManager与FlowManager从Master中的抽取后，下面的工作可以不再与
FlowManager耦合，而是只与ConfigManager耦合。下面要使CentralMaster的设计在Minos系
统中成型。

本期会初步实现Master的多机化，但是 *只支持静态分配传输任务* ，即分配传输任务只发
生在新建日志传输的时候。同时，本期会支持简单的 *负载均衡策略* 。

CentralMaster在整个Minos系统中是唯一的，并且只有它才又权限管理日志配置，以及分配
传输任务。需要要保证它的唯一性（通过在ZK节点上写入自己的ip:port），以及高可用性
（通过主从切换）。

相应地，普通Master也要做好随时升级为CentralMaster的准备。

** 第三步：

* 抽取ConfigManager和FlowManager
** MinosMeta的修改
MinosMeta需要增加下面接口：
#+begin_src cpp
  RegisterMaster();        // 在ZK上创建临时节点，告知系统自己的存在。先定期
                           // Register，防止异常导致session过期而导致节点消失
  ListAllAliveMaster();    // 返回系统所有存活的Master的列表
  UpdateIdListForMaster(); //
  GetIdListByMaster();     // 获取本Master
#+end_src

*** 将CheckpointAccessor从MinosMeta中抽取出去
   SCHEDULED: <2014-03-13 四>
在编码之前，我要先把CheckpointAccessor从MinosMeta中抽取出去，让FlowManager直接调
用CheckpointAccessor来管理传输流的Checkpoint，而不是通过MinosMeta。从此后，
Checkpoint将不再是Minos元数据了，MinosMeta中LogFlow相关的接口也一并删除。

我调查了一下，发现改造代价很小，因为之前我将传输流Checkpoint相关的处理全部封装在
FlowManager的三个函数里面：
#+begin_src cpp
bool ReloadLogFlow(int log_module_id);
bool SaveLogFlow(int log_module_id, bool is_new) const;
bool DestroyLogFlow(int log_module_id);
#+end_src

代码相关耦合仅仅是这三个函数里的四处MinosMeta的方法调用，将它们替换为
CheckpointAccessor的方法调用就可以了。

完成开发和上线。 <2014-03-13 四>

*** TODO 重构FlowManager的单测
处理FlowManager单测耦合就麻烦多了，要修改多处单侧，很繁琐。但是没关系，这是个重构
代码和单测的契机。

简要地评估一下，最好的重构方式是 *完全重写FlowManager的单测* 。我决定将这个工作的
开始时间推迟到本期的末尾。

*** 获取Master的唯一标识
Master要在ZK上写一个带标识的临时来表明自己的存在，而其ip:port端口号称为了最理想的
表示，故现在要写一个获取该标志的函数。考虑到获取ip挺麻烦的，涉及到多网卡的问题，
故获取其hostname。Linux本身提供了 *gethostname(2)* 来获取hostname。我实现的函数接
口如下：
#+begin_src cpp
static bool GetLocalMasterId(std::string* master_id);
#+end_src

*** Master通过在ZK上写临时节点来注册自己
需要实现的接口如下：
#+begin_src cpp
bool MinosMeta::RegisterMaster();
#+end_src

该函数会先判断Master的ZK临时节点是否存在，如果存在，直接删掉。然后再创建一个临时
节点，节点名称为Master的host:port。

*** TODO 定期轮询或者Watch
关键是调用此接口的时机。目前只是在初始化Master时注册自己，以后会加入一个定期轮询
或者Watch节点的机制，来防止节点网络异常而消失。

*** 遍历ZK的/minos/master节点，获取所有当前存活的Master的列表
#+begin_src cpp
  bool MinosMeta::ListAllAliveMasters(std::vector<std::string>* master_id_list);
#+end_src

该函数会根据/minos/master下面的节点列表来返回一个当前存活Master的列表。它是供
ConfigManager来调用，好根据现存的Master来对日志传输任务进行分配。

*** ConfigManager操作/minos/allocation目录
ConfigManager对/minos/allocation目录会进行两种操作：
+ ConfigManager为通过在/minos/allocation目录下相应节点写入日志模块id列表来为各个
  Master分配传输任务
+ 当某Master挂掉后，ConfigManager要将它在/minos/allocation下的节点删除掉

所以，MinosMeta还要提供这些接口：
#+BEGIN_SRC cpp
  bool AllocateIdListForMaster(std::string& master_id,                                               
                               const std::vector<std::string>& log_module_id_list);                  
  bool DeallocateIdListForMaster(std::string& master_id);                                            
#+END_SRC

*** FlowManager访问/minos/allocation目录
多机化之前，FlowManager通过 *GetIdListOfLogConfig()* 来获取 *当前系统所有日志模块
的id列表* 。多机化之后，各FlowManager只关注自己管理的那部分id，通过访问
/minos/allocation目录下的对应节点来获取id列表。所以需要MinosMeta提供如下的接口：
#+BEGIN_SRC cpp
  bool GetIdListByMaster(std::string master_id, std::vector<int>* log_module_id_list);               
  bool GetIdListOfLocalMaster(std::vector<int>* log_module_id_list);     
#+END_SRC

** ConfigService先放在每个Master里面
在原来的设计中，只有CentralMaster才能启动ConfigService，提供日志配置的增删改查操
作。这样的好处是能够让元信息的管理更统一，只有ContralMaster才有权利操纵元数据，坏
处是有一定的改造代价。

于是我在想，是否能让每个Master都启动ConfigService，并提供日志的增删改查操作？这样
可能带来的元信息不一致的问题，思考下面场景：
1. *新增* 。两个Master通知创建一份日志，它们都会尝试获取了系统中最大的
   log_module_id并加1作为新日志的log_module_id，然后它们获得了相等的
   log_module_id。这时候，先执行AddLogConfig成功的日志创建成功，而另外一份会创建
   失败。
2. *删除* 。两个Master同时删除一份日志，但是只有一个删除成功，另外一个删除失败。
3. *修改* 。两个Master同时修改一份日志，两个都会成功，但以最后一次修改为准。

从上面的场景可以看到： *修改* 不会有问题。 *删除* 的话，我修改下删除逻辑，当日志
不存在时直接返回true后也不存在问题。只有 *新建* ，会导致有个Master新建失败，但这
*也不会带来元信息不一致的问题* ，只不过会有几率影响用户体验。考虑到，一个只有一个
Master对外提供HTTP Service服务，所以说， *新建* 问题也不大。

所以说，将ConfigService在每个Master里面启动是完全可行的。
#+BEGIN_EXAMPLE
One Master, One ConfigService.
#+END_EXAMPLE

所以说，目前ConfigManager先不承担ConfigService，而只承担传输任务分配的工作。

* 调整开发计划
** 概述
由于日志数/机器数/节点数原来越多，这周可能要上300份，单Master的随时都可能扛不住。
所以我要加快多Master的开发。我下午想了一下，优化了一下多Master的设计，并于韩超讨
论了一下。优化点包括：
1. 将Master分为三大块：Master本体/ConfigManager/FlowManager（各块的功能下面讲）。
2. 各个Master不在ZK的/minos/master下注册临时节点，而是载下面注册永久节点，并定期
   更新节点内容。节点内容里包括当前时间戳，该Master的负载等（相当于各Master向ZK写
   心跳信息）
3. CentralMaster从/minos/master下获取各Master的状态，然后 *根据时间戳挑出存活的
   Master节点* 以及它们的负载信息，然后进行任务的分配。

Master的三大块：
1. *Master本体* 启动了各个Service以及ConfigManager/FlowManager之后，
   就进入了一个无限循环，在该循环内定时向ZK更新自己的当前状态和更新时间戳。
2. *ConfigManager* 在启动之后，会进入抢主过程，抢到之后成为CentralMaster，负责为
   各个Master分配传输任务。
3. *FlowManager* 从ZK上读取CentralMaster给自己分配的传输任务，开始进行传输流的管
   理，包括新建/删除/更新传输流，通知和报警等等。

** 第一轮改造
*** 基础接口
先完成基础接口的开发/改造和单测。这些接口包括：
#+BEGIN_SRC cpp
bool UpdateMasterStatusById(const std::string& master_id, const MasterStatus& master_status); 
bool ListAllAliveMasters(std::vector<std::string>* master_id_list); 
bool AllocateIdListForMaster(std::string& master_id,                                            
                             const std::vector<int>& log_module_id_list);  
bool GetIdListByMasterId(std::string master_id, std::vector<int>* log_module_id_list); 
bool UpdataMetaDataNotCareExistence(); 
bool DeleteMetaDataNotCareExistence();
#+END_SRC

预计要2小时。

*** Master定期汇报状态
Master中要启动线程定期调用UpdateMasterStatus，目前MasterStatus中只包含当前时间戳。

预计要0.5小时

*** ConfigManager为FlowManager分配任务
再开发ConfigManager。ConfigManager做的事情就是获取或者的Master列表（目前只有1台），
然后为它分配日志模块id（目前所有日志模块id都分配给这单独的一台Master），然后
Master中多启动一个线程来运行ConfigManager，然后上线。

这一轮中，需要简单实现Allocator的框架。

预计要0.5小时

*** 让FlowManager只care自己的传输任务
检测无误后，开始下一轮的开发，这一轮的开发工作是让FlowManager只是在
/minos/allocation下面读取自己的管理的日志模块id（目前看来，改造内容就是将
GetIdListOfLogConfig换成GetIdListOfLocalMaster），然后把Master中除了
ConfigManager外所有使用GetIdListOfLogConfig的地方都换成GetIdListOfLocalMaster。然
后上线。

预计要0.5小时

*** ConfigManager抢注节点
上线检测无误后，开始开发ConfigManager抢注节点的功能。ConfigManager会先抢注
/minos/config-manager节点，抢注成功后，开始工作。

预计要0.5小时
*** 总体时间预估
看来，第一轮改造需要四个小时。我22点开始开发，预计凌晨2点开发完毕。<2014-03-25 二>

** 第二轮：ConfigManager
*** TODO ConfigManager的唯一性保证
ConfigManager还要单独开一个线程专门来判断自己是不是/minos/config-manager节点的主人：
1. 如果是，则要确保自己在工作。如果不在工作，则说明自己非常非常的异常，这时立刻退
   出进程。
2. 如果不是或者节点不存在， *则说明发生了网络异常* 。这时要确保自己不在工作。如果
   还在工作，则会退出工作，并进入抢注节点的无限循环。
#+BEGIN_EXAMPLE
要开一个专门的线程
#+END_EXAMPLE

*** 在ConfigManager中维护着所有的LogConfig
要达成高性能地管理LogConfig，就需要在内存中维护着所有的LogConfig（具体地说，是由
ConfigManager管理），这样就不必每次都访问ZK了。

这样，就需要有一种机制来讲ZK中的/minos/log-config给及时同步到ConfigManager中。很
显然，ZK的Event Watch机制就是用来干这事情的，不过为了达成 *敏捷* ，先用定期轮询
/minos/log-config节点来实现，轮询的时间间隔先设为15秒。
#+BEGIN_EXAMPLE
要开一个专门的同步线程。
#+END_EXAMPLE

用啥数据结构存储这所有的LogConfig？考虑到有查询需求，且日志模块id不连续，故
std::vector不合适，于是使用 *std::map<int, LogConfigMessage>* 来存储，key是日志模
块id。
#+BEGIN_SRC cpp
std::map<int, baidu::minos::LogConfigMessage>
#+END_SRC

*** 各个Master与其管理的日志模块id列表的对应关系
这个对应关系其实就是传输任务分配的结果，会被写到zk的/minos/allocation节点下面，以
让各个Master能获取自己的传输任务。

用map来存储对应关系，如下：
#+BEGIN_SRC cpp
std::map<std::string, std::vector<int> >
#+END_SRC

*** 各日志模块id与其属主Master的对应关系
下面几种场景会用到这个对应关系：
1. Allocator发现有日志被删除时，需要知道该日志的属主Master，然后AllocationMap中删
   掉该日志模块id。
2. 各个Master都会向CentralMaster调用QueryMasterAddress这个RPC接口，故需要在
   ConfigManager中存储日志模块id与属主Master的对应关系以加速查询。
*** 各个Master与其管理的日志的机器列表全集的对应关系
由于目前每个Minos Agent下面的各个节点只会向单个Master更新状态，所以说，传输任务分
配结果必须满足下面的约束：
#+BEGIN_EXAMPLE
单台机器上的所有日志模块必须同时被一个Master管理。
#+END_EXAMPLE

故需要存储对应关系，让Allocator分配新日志时能 *快速判断* 能否分配给某个Master。

用map来存储对应关系，如下：
#+BEGIN_SRC cpp
std::map<std::string, std::set<std::string> >
#+END_SRC

**** 点分十进制还是长整型？
这里用std::string以 *点分十进制* 的方式存储机器ip，而没有用 *长整型* 的方式来存储。
考虑一下Minos系统机器的极限，假设是10W台，每个ip的占12个字节，一共需要120W字节，
一换算，得到 *120K* ，这点内存占用可以接受。

*** 各个noah_node_id与其下面的日志模块id列表中的关系
Minos用户在使用上有个特点，就是按照Noah运维节点的id来查看日志列表，所以
ConfigManager需要维护着noah_node_id与其下面所有日志模块id的对应关系，以方便快速查
询。

用map来存储对应关系，如下：
#+BEGIN_SRC cpp
std::map<int, std::vector<int> >
#+END_SRC

后续会以noah_node_id为key，将该节点下所有日志的状态信息写入到缓存服务器上，以提高
Minos前端页面的反应速度。

*** 传输任务分配规则
*传输任务分配* 是ConfigManager的最主要职责。先列出分配的目的：
#+BEGIN_EXAMPLE
将指定的日志模块id分配最合适的某个Master来管理，并能实现系统的负载均衡。
#+END_EXAMPLE

ConfigManager的可选的Master列表是通过调用 *ListAllAliveMaster()* 接口来获取的，那
么该如何从中为指定日志选择一个合适的Master呢？需要遵循下面规则：
1. 所管理的日志模块id最少的Master优先
2. 所管理的节点数目最少的Master优先
3. CPU占用/内存占用最轻的Master优先
4. 一个带log_tag的日志相当于10份平台的日志
3. 除了被选中的Master外的所有Master下面的节点的机器列表中均不包含该日志的BNS下面的任何一台机器。

*** 加载历史分配结果
每当ConfigManager抢到ZK上的节点后，则会重新从ZK加载历史分配结果。每当触发任务重新
分配时，会将分配结果写到ZK。
*** 更新任务分配map和节点ip map的时机
我在每轮重新分配任务时，都更新一下这两个map。
*** ConfigManager的主循环
在当前的定期轮询模式下的主循环设计：
1. 获取日志模块id的全集（直接从ConfigManager维护的map中取）
2. 获取各Master管理的日志模块id列表
3. 做一轮diff，找出新增和删除的日志模块id
4. 将被删除的日志模块id从该Master的id列表中删除
5. 依次为新增的日志模块id分配一个Master
   1) 判断该日志的机器列表是否有机器是否有所属Master
      - 如果机器列表中有机器属于某Master且只属于这一个Master，则直接将id分配给这个Master，并返回
      - 如果机器列表中的机器属于两台或两台以上的Master，则分配失败，并返回；
      - 如果它们不属于任何一个Master，则进入下一步；
   2) 将这些Master按照负载情况排序
   3) 将id分给负载最轻的Master
   4) 更新Master的负载情况
6. 将Master的分配情况重新写入ZK（只写分配发生改变的Master）

** 第三轮：提供给Agent查询接口
Agent在Master的BNS
#+BEGIN_SRC cpp
service LogFlowService {                                                                               
    rpc QueryMasterAddress(QueryMasterAddressRequest) returns(QueryMasterAddressResponse); 
}
#+END_SRC

#+BEGIN_SRC cpp
message QueryMasterAddressRequest {                                                                    
    required int32 log_module_id = 1;                                                                  
}

message QueryMasterAddressResponse {                                                                   
    required string master_ip = 1;                                                                     
    required int32 master_port = 2;                                                                    
} 
#+END_SRC

** 第四轮：多机情况下的监控和统计

		   
		   
    
